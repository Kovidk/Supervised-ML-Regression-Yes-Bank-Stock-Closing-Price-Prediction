{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "bamQiAODYuh1",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kovidk/Supervised-ML-Regression-Yes-Bank-Stock-Closing-Price-Prediction/blob/main/ML_Regression_Yes_Bank_Stock_Closing_Price_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Yes Bank Stock Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Supervised\n",
        "##### **Contribution**    - Individual\n",
        "#####Team Member 1 - Kovid Krishnan\n",
        "#####Team Member 2 -\n",
        "#####Team Member 3 -\n",
        "#####Team Member 4 -\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "A well-known bank in the Indian financial industry is Yes Bank. The Rana Kapoor fraud case has been in the headlines since 2018 as a result of it. Due to this, it was interesting to observe how it affected the company's stock prices and whether Time series models or other prediction models could properly reflect for such circumstances. Since the bank's foundation, this dataset has included closing, starting, highest, and lowest stock values for each month. The main objective is to predict the stock's closing price of the month.\n",
        "\n",
        "To make our ideas work and gain insights, we need to understand our data well. I've got a clear view of our problem and goal, but there are a few steps left to predict the stock's closing price for a month. I'll do this by using Regression Analysis and building a better model that can quickly and accurately predict what we want.\n",
        "\n",
        "Feature Engineering:- Introducing New Variables, Dummy Variables\n",
        "\n",
        "Univariate Analysis:- Distplot, Histogram, Barplot\n",
        "\n",
        "Bivariate Analysis:- Boxplot, Heatmap\n",
        "\n",
        "Introduced Models:- Linear Regression, Lasso, Ridge and decision tree Regression\n",
        "\n",
        "In our dataset, we've got 185 rows and 5 columns. We focus on two main types of numbers:\n",
        "\n",
        "Dependent Variable (Close): This is like the last price tag of the month's shopping, and we want to predict it.\n",
        "Independent Variables (Open, High, Low): These are like the starting, highest, and lowest prices during the month.\n",
        "\n",
        "Date: Date of record\n",
        "\n",
        "Open: Opening Price of the day\n",
        "\n",
        "High: Highest Price in the day\n",
        "\n",
        "Low: Lowest Price in the day\n",
        "\n",
        "Close: Closing Price of the day"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here. https://github.com/Kovidk/Supervised-ML-Regression-Yes-Bank-Stock-Closing-Price-Prediction/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stockâ€™s closing price of the month.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from datetime import datetime\n",
        "\n",
        "# This technique is used to divide the dataset into a training set and a test set when building the model.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Import libraries for Regressor\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import neighbors\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Import the matrix module to evaluate the model's performance.\n",
        "from sklearn.metrics import *\n",
        "# import pandas.util.testing as tm\n",
        "\n",
        "# Import the variance inflation factor technique to reduce multicollinearity in independent variables.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_df = pd.read_csv(\"/content/drive/MyDrive/AlmaBetter/Cohort London/Project/data_YesBank_StockPrices.csv\")"
      ],
      "metadata": {
        "id": "Kpppw5E9klbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "price_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "price_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "price_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(price_df[price_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "price_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "price_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "price_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date :- It denotes the month and year for a specific pricing.\n",
        "\n",
        "Open :- The price at which a stock started trading that month is referred to as the \"Open.\"\n",
        "\n",
        "High :- The highest price for that particular month.\n",
        "\n",
        "Low :- It describes the monthly minimum price.\n",
        "\n",
        "Close :- It refers to the final trading price for that month, which we have to predict using regression."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for Outliers\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create a boxplot using Seaborn\n",
        "sns.boxplot(data=price_df[['Open', 'Close', 'High', 'Low']], notch=True)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Price Type')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Distribution of Price Types')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vg_JYtYSxOC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Date' column to datetime format using pd.to_datetime()\n",
        "price_df['Date'] = pd.to_datetime(price_df['Date'], format='%b-%y')\n",
        "\n",
        "# Display information about the DataFrame\n",
        "price_df.info()\n"
      ],
      "metadata": {
        "id": "kcDBPlLp48wV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "# Plotting closing prices using Seaborn's lineplot\n",
        "sns.lineplot(data=price_df, x=\"Date\", y=\"Close\", color='blue', label='Closing Price')\n",
        "\n",
        "# Adding scatterplot markers\n",
        "sns.scatterplot(data=price_df, x=\"Date\", y=\"Close\", s=100, color='red', label='Data Points')\n",
        "\n",
        "# Adding grid, labels, and title\n",
        "plt.grid(True)\n",
        "plt.xlabel('Date', fontsize=10)\n",
        "plt.ylabel('Close Prices', fontsize=10)\n",
        "plt.title('Yes Bank Closing Price')\n",
        "\n",
        "# Display the legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dependent Variable - Close"
      ],
      "metadata": {
        "id": "ipIo7iDH0C6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "#distribution of close price stock\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a distribution plot using Seaborn\n",
        "sns.distplot(price_df[\"Close\"], color=\"blue\", bins=20)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of Close Prices')\n",
        "plt.xlabel('Close Price')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Log Transformation to make it a normal distribution of the close price\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a distribution plot of the log-transformed \"Close\" prices using Seaborn\n",
        "sns.distplot(np.log10(price_df[\"Close\"]), color=\"blue\", bins=20)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of Log-Transformed Close Prices')\n",
        "plt.xlabel('Log-Transformed Close Price')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z_8NZVvHxvwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Independent Variable - Open, High, Low"
      ],
      "metadata": {
        "id": "4cdyudKr0vGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Log Transformation to make it a normal distribution of the Open price\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a distribution plot of the log-transformed \"Open\" prices using Seaborn\n",
        "sns.distplot(np.log10(price_df[\"Open\"]), color=\"blue\", bins=20)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of Log-Transformed Open Prices')\n",
        "plt.xlabel('Log-Transformed Open Price')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2R5xJKfE1x1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Log Transformation to make it a normal distribution of the High price\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a distribution plot of the log-transformed \"High\" prices using Seaborn\n",
        "sns.distplot(np.log10(price_df[\"High\"]), color=\"green\", bins=20)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of Log-Transformed High Prices')\n",
        "plt.xlabel('Log-Transformed High Price')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9BLIHSG431Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Log Transformation to make it a normal distribution of the Low price\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a distribution plot of the log-transformed \"Low\" prices using Seaborn\n",
        "sns.distplot(np.log10(price_df[\"Low\"]), color=\"Red\", bins=20)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Distribution of Log-Transformed Low Prices')\n",
        "plt.xlabel('Log-Transformed Low Price')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YkwiYv3P4Gr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relationship between Dependent and Independent Variable"
      ],
      "metadata": {
        "id": "C0zF6Egm45D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# finding relationship between dependent variable - Close and Independent Variable - Open\n",
        "\n",
        "# Create a scatter plot and regression line\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data=price_df, x='Open', y='Close', alpha=0.2, color = 'blue')\n",
        "sns.regplot(data=price_df, x='Open', y='Close', scatter = False, color='violet')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Open')\n",
        "plt.ylabel('Close')\n",
        "plt.title('Scatter Plot and Regression Line of Open vs Close')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding relationship between dependent variable - Close and Independent Variable - High\n",
        "\n",
        "# Create a scatter plot and regression line\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data=price_df, x='High', y='Close', alpha=0.2, color = 'blue')\n",
        "sns.regplot(data=price_df, x='High', y='Close', scatter = False, color='darkgreen')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('High')\n",
        "plt.ylabel('Close')\n",
        "plt.title('Scatter Plot and Regression Line of High vs Close')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GbalkgsR9Cjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding relationship between dependent variable - Low and Independent Variable - Close\n",
        "\n",
        "# Create a scatter plot and regression line\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data=price_df, x='Low', y='Close', alpha=0.2, color = 'blue')\n",
        "sns.regplot(data=price_df, x='Low', y='Close', scatter = False, color='Red')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Low')\n",
        "plt.ylabel('Close')\n",
        "plt.title('Scatter Plot and Regression Line of Low vs Close')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "96kU-hcu9c7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Heatmap"
      ],
      "metadata": {
        "id": "Wc5gjSdOJYp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation = price_df.corr()\n",
        "\n",
        "# Create a heatmap with customized settings\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')\n",
        "\n",
        "# Set title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pair Plot"
      ],
      "metadata": {
        "id": "tjLPJpkwL0DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Using pair plot to analyse relationship between variables\n",
        "\n",
        "# Create a pair plot\n",
        "pair_plot = sns.pairplot(price_df, kind=\"scatter\", diag_kind=\"hist\", corner=True, markers='o', palette='coolwarm')\n",
        "\n",
        "# Customize titles and labels\n",
        "pair_plot.fig.suptitle(\"Pairwise Relationships\", y=1.02)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 6"
      ],
      "metadata": {
        "id": "Q4YJNqN9oPMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multicollinearity"
      ],
      "metadata": {
        "id": "bBAQsRccQbbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the variation inflation factor (VIF), To determine the correlation between independent variables."
      ],
      "metadata": {
        "id": "in5Nr6aeRPEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_to_exclude = [\"Date\", \"Close\"]\n",
        "independent_variables = [col for col in price_df.columns if col not in column_to_exclude]\n",
        "dependent_variable = [\"Close\"]\n",
        "\n",
        "# Select independent variables\n",
        "X = price_df[independent_variables]\n",
        "\n",
        "# Calculate VIF for each variable\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Variable\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "2g4Me6iPRQkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Even though some VIF ratings are high, we've decided not to change the features as each feature is really important for our predictions, just like how real-world indicators use them to guess future values.\n",
        "\n",
        "*  We're not getting rid of any columns because every single one is super important for predictions. We'll see how well the model works, even with the features kind of overlapping."
      ],
      "metadata": {
        "id": "B5Sb1-u-UbNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating arrays of our input variable and label to feed the data to the model.\n",
        "# Create the data of independent variables\n",
        "x = np.log10(price_df[independent_variables]).values            # applying log transform on our independent variables.\n",
        "\n",
        "# Create the dependent variable data\n",
        "y = np.log10(price_df[dependent_variable]).values               # applying log transform on our dependent variable."
      ],
      "metadata": {
        "id": "7lO-cpIeb7TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into train and test datasets\n",
        "# Train and Test Set data splited into 70-30\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3,random_state= 1)"
      ],
      "metadata": {
        "id": "PHF741t7YE6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "NPrSE_dCYou7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "7t8NWaDHYuGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data\n",
        "scaler = MinMaxScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Split our data into train and test datasets\n",
        "# Train and Test Set data splited into 70-30\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3,random_state= 1)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will Create 4 Regression Models for our Data. 1} Linear Regression 2} Lasso Regression 3} Ridge Regression 4} Decision Tree Regression"
      ],
      "metadata": {
        "id": "M0euMrmjdKm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1\n",
        "### Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation of Linear Regression\n",
        "\n",
        "# Initializing the model.\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Fitting the model on our train data.\n",
        "model_lr.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "JBdpbTiGUHNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on our test data.\n",
        "y_pred_linear = model_lr.predict(x_test)\n",
        "y_pred_linear[:10]"
      ],
      "metadata": {
        "id": "x-L5nTcaUI8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters. printing the intercept.\n",
        "model_lr.intercept_\n"
      ],
      "metadata": {
        "id": "neMBJMHCUMLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the model coefficients.\n",
        "model_lr.coef_"
      ],
      "metadata": {
        "id": "s1t8rDdqUPoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "MAE_linear = round(mean_absolute_error(10**(y_test),(10**y_pred_linear)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_linear}\")\n",
        "\n",
        "MSE_linear = round(mean_squared_error((10**y_test),10**(y_pred_linear)),4)\n",
        "print(f\"Mean squared Error : {MSE_linear}\")\n",
        "\n",
        "RMSE_linear = round(np.sqrt(MSE_linear),4)\n",
        "print(f\"Root Mean squared Error : {RMSE_linear}\")\n",
        "\n",
        "R2_linear = round(r2_score(10**(y_test), 10**(y_pred_linear)),4)\n",
        "print(f\"R2 score : {R2_linear}\")\n",
        "\n",
        "Adjusted_R2_linear = round(1-(1-r2_score(10**y_test,10**y_pred_linear))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),3)\n",
        "print(f\"Adjusted R2 score : {Adjusted_R2_linear}\")\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing our performance data for this model so that we can compare them with other models. Let's store them in a dict for now.\n",
        "linear_regessor_list = {'Mean Absolute Error' : MAE_linear,'Mean squared Error' : MSE_linear,\n",
        "                   'Root Mean squared Error' : RMSE_linear,'R2 score' : R2_linear,'Adjusted R2 score' : Adjusted_R2_linear }\n",
        "\n",
        "# converting above dict into a dataframe\n",
        "metric_df = pd.DataFrame.from_dict(linear_regessor_list, orient='index').reset_index()\n",
        "\n",
        "# renaming the columns.\n",
        "metric_df = metric_df.rename(columns={'index':'Metric',0:'Linear Regression'})\n",
        "metric_df"
      ],
      "metadata": {
        "id": "ydQoozwaz5e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sctter Plot of Actual vs Predicted Values\n",
        "plt.scatter(y_test,y_pred_linear)\n",
        "plt.xlabel('Actual test value')\n",
        "plt.ylabel('Predicted test value')"
      ],
      "metadata": {
        "id": "jchGjLR-f4cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Price vs Prediction price for Linear Regression plot:\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(10**(y_pred_linear))\n",
        "plt.plot(10**(np.array(y_test)))\n",
        "plt.xlabel(\"No. of test data\",fontsize= 12)\n",
        "plt.ylabel(\"Price\",fontsize= 12)\n",
        "plt.suptitle(\"Actual Stock Close Price VS Predicted Stock Close Price\",fontsize=14)\n",
        "plt.legend([\"Predicted\",\"Actual\"],fontsize= 12)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p6XvWOGfgBXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Lasso Regression"
      ],
      "metadata": {
        "id": "_Ahoa6UyxaDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model with some base values.\n",
        "lasso  = Lasso(alpha=0.0001 , max_iter= 3000)\n",
        "# Fitting the model on our training data.\n",
        "lasso.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "-zWtoUTqxfkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the intercept and coefficients.\n",
        "lasso.intercept_"
      ],
      "metadata": {
        "id": "qwBLyQaVxmzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef_"
      ],
      "metadata": {
        "id": "euO3ICnrxpSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "efnzhIYFUR-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation. optimizing our model by finding the best value of our hyperparameter.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lasso_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.005,0.006,0.007,0.01,0.015,0.02,1e-1,1,5,10,20,30,40,45,50]}  # list of parameters.\n",
        "\n",
        "lasso_regressor = GridSearchCV(lasso, lasso_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "g4Z6_-cjxsXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best parameter\n",
        "lasso_regressor.best_params_          # after several iterations and trials, we get this value as best parameter"
      ],
      "metadata": {
        "id": "_Wc4xUg2x4Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score\n",
        "lasso_regressor.best_score_"
      ],
      "metadata": {
        "id": "0AC18t12x9U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting on the test dataset.\n",
        "y_pred_lasso = lasso_regressor.predict(x_test)\n",
        "print(y_pred_lasso)"
      ],
      "metadata": {
        "id": "jnNe_mF3yatZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Mean Absolute Error\n",
        "MAE_lasso = round(mean_absolute_error(10**(y_test), 10**(y_pred_lasso)), 4)\n",
        "print(f\"Mean Absolute Error: {MAE_lasso}\")\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "MSE_lasso = round(mean_squared_error(10**(y_test), 10**(y_pred_lasso)), 4)\n",
        "print(f\"Mean Squared Error: {MSE_lasso}\")\n",
        "\n",
        "# Calculate Root Mean Squared Error\n",
        "RMSE_lasso = round(np.sqrt(MSE_lasso), 4)\n",
        "print(f\"Root Mean Squared Error: {RMSE_lasso}\")\n",
        "\n",
        "# Calculate R2 Score\n",
        "R2_lasso = round(r2_score(10**(y_test), 10**(y_pred_lasso)), 4)\n",
        "print(f\"R2 score: {R2_lasso}\")\n",
        "\n",
        "# Calculate Adjusted R2 Score\n",
        "Adjusted_R2_lasso = round( 1 - (1 - r2_score(10**y_test, 10**y_pred_lasso)) * ((x_test.shape[0] - 1) / (x_test.shape[0] - x_test.shape[1] - 1)),4 )\n",
        "\n",
        "print(f\"Adjusted R2 score: {Adjusted_R2_lasso}\")"
      ],
      "metadata": {
        "id": "opuQHoluyfra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now saving these metrics to our metrics dataframe. First we save them in a list and then we pass them to the df.\n",
        "metric_df['Lasso'] = [MAE_lasso, MSE_lasso, RMSE_lasso, R2_lasso, Adjusted_R2_lasso]"
      ],
      "metadata": {
        "id": "TR48sV8nyzkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the predicted values vs actual.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel(\"No. of test data\",fontsize= 12)\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Lasso regression\")"
      ],
      "metadata": {
        "id": "-UdPmSC0y7D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Ridge Regression"
      ],
      "metadata": {
        "id": "OXgvzZ5N2BcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# importing ridge regressor model.\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# alpha: Regularization strength\n",
        "ridge = Ridge(alpha=1.0)  # We can adjust the value of alpha based on your needs\n",
        "\n",
        "# Fitting the Ridge model on the training data\n",
        "ridge.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiating the parameter grid for alpha (regularization strength).\n",
        "ridge_param_grid = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,0.3,0.7,1,1.2,1.33,1.365,1.37,1.375,1.4,1.5,1.6,1.8,2.5,5,10,20,30,40,45,50,55,60,100]}\n",
        "\n",
        "# cross validation.\n",
        "ridge_regressor = GridSearchCV(ridge, ridge_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "vXG81cmFVvkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter value (for alpha)\n",
        "ridge_regressor.best_params_"
      ],
      "metadata": {
        "id": "p8cvEK0GPU90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best score for optimal value of alpha.\n",
        "ridge_regressor.best_score_"
      ],
      "metadata": {
        "id": "KLmsXqQHPX9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on the test dataset now.\n",
        "y_pred_ridge = ridge_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "w7gz5uFNPZFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating performance.\n",
        "MAE_ridge = round(mean_absolute_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_ridge}\")\n",
        "\n",
        "MSE_ridge  = round(mean_squared_error(10**(y_test),10**(y_pred_ridge)),4)\n",
        "print(\"Mean squared Error :\" , MSE_ridge)\n",
        "\n",
        "RMSE_ridge = round(np.sqrt(MSE_ridge),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_ridge)\n",
        "\n",
        "R2_ridge = round(r2_score(10**(y_test), 10**(y_pred_ridge)),4)\n",
        "print(\"R2 score :\" ,R2_ridge)\n",
        "\n",
        "Adjusted_R2_ridge = round(1-(1-r2_score(10**y_test, 10**y_pred_ridge))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_ridge)"
      ],
      "metadata": {
        "id": "gwThLTiTPbpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these values in a list and appending to our metric df.\n",
        "ridge_regressor_list = [MAE_ridge,MSE_ridge,RMSE_ridge,R2_ridge,Adjusted_R2_ridge]\n",
        "metric_df['Ridge'] = ridge_regressor_list"
      ],
      "metadata": {
        "id": "6PZ9EpZ5Pe0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting predicted and actual target variable values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.xlabel(\"No. of test data\",fontsize= 12)\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Ridge regression\")"
      ],
      "metadata": {
        "id": "Yy8ZcZpqPhYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "UbqeA80QP23D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Elastic-Net Regression"
      ],
      "metadata": {
        "id": "1TMwIKMwQNrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing and initializing Elastic-Net Regression.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "\n",
        "# initializing parameter grid.\n",
        "elastic_net_param_grid = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,0.001,0.01,0.02,0.03,0.04,1,5,10,20,40,50,60,100],\n",
        "                          'l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\n",
        "\n",
        "# cross-validation.\n",
        "elasticnet_regressor = GridSearchCV(elasticnet, elastic_net_param_grid, scoring='neg_mean_squared_error',cv=5)\n",
        "elasticnet_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "y_drnJx_QTIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "EzAs0w1QeZ70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameter\n",
        "elasticnet_regressor.best_params_"
      ],
      "metadata": {
        "id": "z2FURo38QW8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best score for the optimal parameter.\n",
        "elasticnet_regressor.best_score_"
      ],
      "metadata": {
        "id": "-jI2XDZrQ7LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making the predictions.\n",
        "y_pred_elastic_net = elasticnet_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "8hkd_cHbQ_7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAE_elastic_net = round(mean_absolute_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(f\"Mean Absolute Error : {MAE_elastic_net}\")\n",
        "\n",
        "MSE_elastic_net  = round(mean_squared_error(10**(y_test),10**(y_pred_elastic_net)),4)\n",
        "print(\"Mean squared Error :\" , MSE_elastic_net)\n",
        "\n",
        "RMSE_elastic_net = round(np.sqrt(MSE_elastic_net),4)\n",
        "print(\"Root Mean squared Error :\" ,RMSE_elastic_net)\n",
        "\n",
        "R2_elastic_net = round(r2_score(10**(y_test), (10**y_pred_elastic_net)),4)\n",
        "print(\"R2 score :\" ,R2_elastic_net)\n",
        "\n",
        "Adjusted_R2_elastic_net = round(1-(1-r2_score(10**y_test, 10**y_pred_elastic_net))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)),4)\n",
        "print(\"Adjusted R2 score: \", Adjusted_R2_elastic_net)"
      ],
      "metadata": {
        "id": "h137aWpyRCh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing these metrics in our dataframe.\n",
        "elastic_net_metric_list = [MAE_elastic_net,MSE_elastic_net,RMSE_elastic_net,R2_elastic_net,Adjusted_R2_elastic_net]\n",
        "metric_df['Elastic Net'] = elastic_net_metric_list"
      ],
      "metadata": {
        "id": "Gpist7l8REfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us plot the actual and predicted target variables values.\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.plot(np.array(10**y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel(\"No. of test data\",fontsize= 12)\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title(\"Actual vs Predicted Closing price Elastic Net regression\")"
      ],
      "metadata": {
        "id": "q6L7Rx4KRIKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing the performance of all models that we have implemented.\n",
        "metric_df.T"
      ],
      "metadata": {
        "id": "FY0-AY5eRPrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above data, we can clearly see that the best performing model is Elastic Net as it scores the best in every single metric."
      ],
      "metadata": {
        "id": "-4ZNbfK1Rra3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the predicted values of all the models against the true values.\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(10**y_test, linewidth=2)\n",
        "plt.plot(10**y_pred_linear)\n",
        "plt.plot(10**y_pred_lasso)\n",
        "plt.plot(10**y_pred_ridge)\n",
        "plt.plot(10**y_pred_elastic_net)\n",
        "plt.legend(['linear','lasso','ridge','elastic_net'])\n",
        "plt.xlabel(\"No. of test data\",fontsize= 12)\n",
        "plt.ylabel(\"Price\")\n",
        "plt.title('Actual vs Predicted Closing Price values by various Algorithms', weight = 'bold',fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i32z3WdQRQOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finding the best score for the optimal parameter.\n",
        "elasticnet_regressor.best_score_"
      ],
      "metadata": {
        "id": "AqH5vGqxQZ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elasticnet_regressor.best_score_"
      ],
      "metadata": {
        "id": "6Aezn6vNeVym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By utilizing data visualization on our target variable, we can clearly observe the substantial impact of the 2018 fraud case involving Rana Kapoor, as evidenced by a significant drop in stock prices during that period.\n",
        "\n",
        "* Upon loading the dataset, we discovered that it is free from null values and duplicate entries, ensuring data integrity.\n",
        "\n",
        "* While some outliers are present in our features, given the modest size of the dataset, removing these instances would result in valuable information loss.\n",
        "\n",
        "* We noted that the distribution of all variables displays positive skewness. To address this, we applied log transformations to the variables.\n",
        "\n",
        "* A noticeable correlation exists between the dependent and independent variables. This suggests that our dependent variable significantly relies on our features and can be accurately predicted using them.\n",
        "\n",
        "* A relatively high correlation was observed among our independent variables. Although this multicollinearity is inevitable due to the small dataset size, it's worth noting.\n",
        "\n",
        "* We implemented various models on our dataset to predict closing prices. Interestingly, all models demonstrated remarkable performance. Among them, the Elastic Net regressor emerged as the top performer, boasting an Adjusted R-squared score of 0.9932 and excelling in all evaluation metrics.\n",
        "\n",
        "* All deployed models exhibited impressive results on our data, yielding an Adjusted R-squared exceeding 99%.\n",
        "\n",
        "* With our model achieving such precise predictions, we can confidently deploy it for further predictive tasks involving future data"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}